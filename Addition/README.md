This project tests the capability of GPT2 on 2 to 18 digit addition. The model weights were not saved but the accuracy for each addition was noted. The packages used are all of latest versions. 

# References:
The source code was modified to our specific use case from the following:
  - https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py
  - https://github.com/quentinlintz/synthetic-data-generator
