{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f488666-2b1d-4273-b7a6-cba976c4c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install libraries\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44137429-11c3-4921-a0bf-7bdfe940e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91203a87da0a4f95bfa6e3de65cb16c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a4fdbe751346b8a9063ad031d62fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be368bccdf9849d38bc807b927dd961b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16324bfd0ee4cb7bdb34b2e4f8f6cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef5aeeb5ef94df19276c61dad283fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1cdffad06244ad9d3319da34522e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load our training data\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='finetune_dataset.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='finetune_dataset_val.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faf21a48-9e1d-4ec2-8940-15d65ab94d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['num1', 'num2', 'result'],\n",
       "    num_rows: 6400\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure our data loaded\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa045f3-e79d-4d08-855c-0e5abb42a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting function - this turns the examples from our jsonl files into text for our model to train on\n",
    "def formatting_func(example):\n",
    "    text = f\"### answer the following:\\n{example['num1']} + {example['num2']}\\na: {example['result']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978af1dc-c1db-4328-a677-f747412522be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f24bfff765c46a285fb113eabe071e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f00bb583142f493f4127814fe42e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3228642a3e541f8bf4dd43223429a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eea3bfcb06d460db8c695b6a0b33170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c8c591cfc140058205fb62608d45aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546a00a56063476f9165ee67c43042fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266f42973d00444b8872ae666fd6706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load base model - mistralai/Mistral-7B-Instruct-v0.1 - using 4-bit quantization\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bfb9fa-3af8-436a-af99-4bb35343f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32794cdafde749cab01791d259fd9dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492cfa5a3db74b2aa028c9513ccc4919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91855cd0736c4741963b3dd50144c1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8bab4a7707430ebb0dddf58f2f1262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7f1866-5111-4c86-a235-bcff09c59b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27644103f92c4b7390f4a3cd3d19c61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d2025d6209429aa04b713d0e96594c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# format prompt and tokenize samples\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6b2b48-a090-4cd9-b750-6689046157e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize distribution of example lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf31a27-1ffc-4209-ba32-957d2cc95176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our max token length, and tokenize our examples accordingly\n",
    "max_length = 40\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "078cb3fb-1be5-41be-abdf-2d35d7d4497d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c6a1433a1041298d287ac9e57c1b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcefeefc8b5c457ba8619f06ab90ce38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c3e9fc-0f13-480a-856f-0bb1e6bdb8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 4372, 272, 2296, 28747, 13, 28783, 28750, 28734, 28734, 28787, 28783, 648, 28705, 28740, 13, 28708, 28747, 28705, 28783, 28750, 28734, 28734, 28787, 28774, 2]\n"
     ]
    }
   ],
   "source": [
    "# check that prompts are formatted correctly\n",
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0f0867-854a-4c6f-ae9b-a6e18a787a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all should be the same length now\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e0480c-0702-4e65-9dd5-0199e5a352e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test base Mistral on an example\n",
    "eval_prompt = \" Answer the following:\\n38 + 11\\na:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15743603-a17d-4912-9365-177f8d738c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer the following:\n",
      "38 + 11\n",
      "a: 49\n"
     ]
    }
   ],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aebf03f7-4c4c-42d1-a6e1-ef6f504d0b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up LoRA\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac860dd-7a05-43fb-abf2-91c09b5a8432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3eedf3-2b7f-4bd2-9124-f93597040dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337cae07-b232-4265-9984-f17a60b474c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f4eb57-7d5a-411c-861d-10cbd8dbf17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447f8383-fbe1-46ca-8c41-c293776bd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160e39ad-5220-4848-8daa-dda0c4b1e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fc9207-ff7b-4b94-ba0a-6ba42244807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wandb -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f3ddec9-da26-40ca-948c-96be7fc46099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db8bf93f-9189-4069-8403-2001af2e39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d965a54d-fcd4-4b70-b05d-617cb9428982",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = \"math-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87b7eb71-3acc-4ad4-b257-010059839780",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "229719b5-eeb0-4bad-93ce-9d1d6fddaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7ba11c5-e8dd-4433-a8d9-a15fc6400d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 23:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.125200</td>\n",
       "      <td>1.312251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.347100</td>\n",
       "      <td>1.187280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.122300</td>\n",
       "      <td>1.020326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.072700</td>\n",
       "      <td>1.181343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.255200</td>\n",
       "      <td>1.154048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.485400</td>\n",
       "      <td>1.374397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.173500</td>\n",
       "      <td>0.941540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.940093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.927202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.920700</td>\n",
       "      <td>0.917826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.899700</td>\n",
       "      <td>0.915063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.905500</td>\n",
       "      <td>0.911883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>0.903760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.932400</td>\n",
       "      <td>0.922557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.008600</td>\n",
       "      <td>1.016597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.017000</td>\n",
       "      <td>1.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.988902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.985900</td>\n",
       "      <td>0.982799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>0.990103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.003700</td>\n",
       "      <td>0.987752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.1000918807983397, metrics={'train_runtime': 1432.9534, 'train_samples_per_second': 0.698, 'train_steps_per_second': 0.349, 'total_flos': 1726968299520000.0, 'train_loss': 1.1000918807983397, 'epoch': 0.16})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"math-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7db7fb5-4321-4140-ae71-69991a99c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec8be0cbdf841ca8aa7c2472e65074c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up for inference on the fine-tuned model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d783b1e8-7a97-4619-ba7d-061cfad335de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-math-finetune/checkpoint-325\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe4072db-6ade-4ae4-9424-4e219693ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer the following:\n",
      "363 + 126\n",
      "a: 490\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" Answer the following:\\n363 + 126\\na:\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b138c0e5-2afc-450c-a596-9672e134402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "def extract_numbers_from_string(content):\n",
    "    # Regex patterns to match numbers in the specified format\n",
    "    addition_pattern = r'(\\d+)\\s*\\+\\s*(\\d+)'\n",
    "    a_number_pattern = r'a:\\s*(\\d+)'\n",
    "    \n",
    "    # Find and extract numbers for the addition\n",
    "    addition_match = re.search(addition_pattern, content)\n",
    "    if addition_match:\n",
    "        num1, num2 = addition_match.groups()\n",
    "    else:\n",
    "        num1, num2 = None, None\n",
    "\n",
    "    # Find and extract the number following 'a:'\n",
    "    a_number_match = re.search(a_number_pattern, content)\n",
    "    if a_number_match:\n",
    "        a_number = a_number_match.group(1)\n",
    "    else:\n",
    "        a_number = None\n",
    "\n",
    "    return int(num1), int(num2), int(a_number)\n",
    "\n",
    "def run_problem(num1, num2):\n",
    "    eval_prompt = f\" Answer the following:\\n{num1} + {num2}\\na:\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def generate_random_n_digit_number(n):\n",
    "    \"\"\"\n",
    "    Generates a random n-digit number.\n",
    "\n",
    "    :param n: Number of digits in the number.\n",
    "    :return: A random n-digit number.\n",
    "    \"\"\"\n",
    "    # Ensure the first digit is not zero\n",
    "    first_digit = random.randint(1, 9)\n",
    "\n",
    "    # Generate the remaining n-1 digits, which can include zero\n",
    "    remaining_digits = [random.randint(0, 9) for _ in range(n - 1)]\n",
    "\n",
    "    # Combine the digits to form the number\n",
    "    digits = [first_digit] + remaining_digits\n",
    "    return int(''.join(map(str, digits)))\n",
    "\n",
    "def get_random_pair(used_pairs: set, i: int, j: int) -> Tuple[int, int]:\n",
    "    nums_used = True\n",
    "    while nums_used:\n",
    "        num1 = generate_random_n_digit_number(i)\n",
    "        num2 = generate_random_n_digit_number(j)\n",
    "        pair = (num1, num2)\n",
    "        nums_used = pair in used_pairs\n",
    "    used_pairs.add(pair)\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad5a19b-3f71-4424-8bdf-ee47d5f75bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338315 7971 346286 True\n"
     ]
    }
   ],
   "source": [
    "used_pairs = set()\n",
    "num1, num2 = get_random_pair(used_pairs, 6, 4)\n",
    "response = run_problem(num1, num2)\n",
    "_, _, answer = extract_numbers_from_string(response)\n",
    "print(num1, num2, answer, answer == (num1 + num2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aeb94d3-8418-464c-b68d-65d028a79da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f795f227-c312-4977-930f-c1e1d61f5421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934266 8 934274 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392591 8 392600 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249909 3 249912 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809366 6 809372 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424574 8 424582 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636416 8 636424 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909145 1 909146 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532543 7 532550 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339728 2 339730 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388794 7 388791 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519532 23 519555 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527001 36 527037 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353363 19 353382 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171915 68 172003 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809244 13 809257 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870139 66 870205 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596627 19 596646 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126221 98 126319 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437765 90 438255 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247704 88 247792 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536124 362 536486 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756580 462 756622 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740700 894 741594 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560222 916 561138 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660933 735 661668 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926732 935 927607 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854533 812 855345 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400434 395 400829 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189175 258 189433 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399036 608 399194 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697650 1685 699335 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294985 5644 300629 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "841263 1526 842790 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405218 4680 410908 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168991 3992 172983 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182452 9338 191790 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342669 5756 348425 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471427 5835 477262 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406737 3960 410697 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372282 9203 381485 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919787 63848 983635 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514191 13407 527608 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373073 30878 403951 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809980 18308 828288 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402821 47289 450100 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551313 66285 617598 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306337 15238 321575 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472273 13192 485465 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144567 49835 194302 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993290 49416 1042706 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264807 521770 786577 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117969 161289 289258 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581937 783016 1364953 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849124 664426 1513550 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993137 807697 1800834 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546036 417525 963561 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697948 328304 1026252 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738290 982212 1720502 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368534 435159 703693 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491858 240626 732484 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "used_pairs = set()\n",
    "results = defaultdict(list)\n",
    "for j in range(1, 7):\n",
    "    for k in range(100):\n",
    "        num1, num2 = get_random_pair(used_pairs, 6, j)\n",
    "        response = run_problem(num1, num2)\n",
    "        _, _, answer = extract_numbers_from_string(response)\n",
    "        is_correct = answer == (num1 + num2)\n",
    "        results[j].append((num1, num2, answer, is_correct))\n",
    "        if k % 10 == 0:\n",
    "            print(num1, num2, answer, is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2336c70c-f314-4cb7-a4a6-9c76639f4aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94, 89, 79, 77, 81, 75]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for j in range(1, 7):\n",
    "    acc = sum([1 for r in results[j] if r[3]])\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f260b4df-d4e5-4f95-94d4-6001b250611a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
