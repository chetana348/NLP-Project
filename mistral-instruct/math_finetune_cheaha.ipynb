{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9db2bb",
   "metadata": {},
   "source": [
    "The code here is adapted from the brev.dev Mistral fine-tuning tutorial by Harper Caroll at\n",
    "\n",
    "https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ce2e2",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7ff37-34ea-48bb-9eb5-db1ef8bc76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if pytorch needs to be installed in your conda env\n",
    "!conda install --yes pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a21228b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/oblaat/.local/lib/python3.11/site-packages (0.41.2.post2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b9cddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - pytorch\n",
      " - nvidia\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a5b394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe064ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592f639a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - pytorch\n",
      " - nvidia\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e89cae21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - pytorch\n",
      " - nvidia\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a16a1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - pytorch\n",
      " - nvidia\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7578ebdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - pytorch\n",
      " - nvidia\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589729bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627ead37-5ebd-4d42-b020-b6d1a8f26aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1 /home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__, torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f488666-2b1d-4273-b7a6-cba976c4c785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#install libraries\n",
    "#!pip install -q -U bitsandbytes\n",
    "#!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "#!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "#!pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44137429-11c3-4921-a0bf-7bdfe940e389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load our training data\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='finetune_dataset.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='finetune_dataset_val.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf21a48-9e1d-4ec2-8940-15d65ab94d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['num1', 'num2', 'result'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure our data loaded\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa045f3-e79d-4d08-855c-0e5abb42a670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# formatting function - this turns the examples from our jsonl files into text for our model to train on\n",
    "def formatting_func(example):\n",
    "    text = f\"### answer the following:\\n{example['num1']} + {example['num2']}\\na: {example['result']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e3152c-8152-45ae-9ed3-ba42007023ee",
   "metadata": {},
   "source": [
    "WARNING: you need at least 16GB CPU RAM for this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978af1dc-c1db-4328-a677-f747412522be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c40b9ce7ab54d65b4f4779c1b7e10b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fadb47ef1fd469bb3065791b4f6f47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load base model - mistralai/Mistral-7B-Instruct-v0.1 - using 4-bit quantization\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bfb9fa-3af8-436a-af99-4bb35343f86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f035e4b8cebb44c7b47fa6f5787d94b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbbf99a094e4d949c9f63daeb9ba7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705bdbe3f3744506a8118f92d899fd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26a02feffd8491fa72b8ce27b2f9aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7f1866-5111-4c86-a235-bcff09c59b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ca82a18c00487081438924e8dec9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99f3fc04f6f4480b92428f7c45b475f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# format prompt and tokenize samples\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce6b2b48-a090-4cd9-b750-6689046157e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize distribution of example lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf31a27-1ffc-4209-ba32-957d2cc95176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set our max token length, and tokenize our examples accordingly\n",
    "max_length = 40\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "078cb3fb-1be5-41be-abdf-2d35d7d4497d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8b561b7ad0481c81025fdf9a6844b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b2878b8f0c4a109594c77bda9651ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c3e9fc-0f13-480a-856f-0bb1e6bdb8be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 774, 4372, 272, 2296, 28747, 13, 28781, 28750, 28750, 28740, 28740, 28784, 648, 28705, 28770, 13, 28708, 28747, 28705, 28781, 28750, 28750, 28740, 28740, 28774, 2]\n"
     ]
    }
   ],
   "source": [
    "# check that prompts are formatted correctly\n",
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0f0867-854a-4c6f-ae9b-a6e18a787a79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all should be the same length now\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e0480c-0702-4e65-9dd5-0199e5a352e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test base Mistral on an example\n",
    "eval_prompt = \" Answer the following:\\n38 + 11\\na:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15743603-a17d-4912-9365-177f8d738c34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer the following:\n",
      "38 + 11\n",
      "a: 49\n"
     ]
    }
   ],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aebf03f7-4c4c-42d1-a6e1-ef6f504d0b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up LoRA\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aac860dd-7a05-43fb-abf2-91c09b5a8432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3eedf3-2b7f-4bd2-9124-f93597040dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "337cae07-b232-4265-9984-f17a60b474c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9f4eb57-7d5a-411c-861d-10cbd8dbf17a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm()\n",
      "            (post_attention_layernorm): MistralRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f71c19",
   "metadata": {},
   "source": [
    "set up Accelerator, this part might not really be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447f8383-fbe1-46ca-8c41-c293776bd623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "160e39ad-5220-4848-8daa-dda0c4b1e4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7cad4",
   "metadata": {},
   "source": [
    "Some code that has been commented out below is related to W&B monitoring. You can reenable it if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1fc9207-ff7b-4b94-ba0a-6ba42244807c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q wandb -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f3ddec9-da26-40ca-948c-96be7fc46099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import wandb, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db8bf93f-9189-4069-8403-2001af2e39be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d965a54d-fcd4-4b70-b05d-617cb9428982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb_project = \"math-finetune\"\n",
    "#if len(wandb_project) > 0:\n",
    "#    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87b7eb71-3acc-4ad4-b257-010059839780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "229719b5-eeb0-4bad-93ce-9d1d6fddaf11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7ba11c5-e8dd-4433-a8d9-a15fc6400d90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 09:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.996000</td>\n",
       "      <td>1.204819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.977300</td>\n",
       "      <td>0.838673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.836631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.825600</td>\n",
       "      <td>0.830397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.832961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.825722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.843400</td>\n",
       "      <td>0.822375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.837100</td>\n",
       "      <td>0.820333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.818811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.812267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.814662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>0.809902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.810337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.809800</td>\n",
       "      <td>0.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.808047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.806731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.795800</td>\n",
       "      <td>0.807754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.807312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.807228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.807056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/oblaat/.conda/envs/jack_conda_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/oblaat/.local/lib/python3.11/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.8827005729675294, metrics={'train_runtime': 566.3493, 'train_samples_per_second': 1.766, 'train_steps_per_second': 0.883, 'total_flos': 1726968299520000.0, 'train_loss': 0.8827005729675294, 'epoch': 0.25})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"math-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=500,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        #report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2891f",
   "metadata": {},
   "source": [
    "At this point you probably want to refresh the iPython kernel to clear RAM before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7db7fb5-4321-4140-ae71-69991a99c2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e525a3bd64c478885c88c88a5ac6851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up for inference on the fine-tuned model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    #use_auth_token=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7d14c",
   "metadata": {},
   "source": [
    "Here we apply the LoRA adapter to the base model. The first time I ran this I chose checkpoint 325 because loss started increasing on both training and validation after that. Here it looks like losses keep getting lower, so let's take the end at checkpoint 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d783b1e8-7a97-4619-ba7d-061cfad335de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-math-finetune/checkpoint-500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471cb16",
   "metadata": {},
   "source": [
    "Testing the adapted model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe4072db-6ade-4ae4-9424-4e219693ef68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer the following:\n",
      "363 + 126\n",
      "a: 489\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \" Answer the following:\\n363 + 126\\na:\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d5cb26",
   "metadata": {},
   "source": [
    "Some functions to help with batch testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b138c0e5-2afc-450c-a596-9672e134402f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "def extract_numbers_from_string(content):\n",
    "    # Regex patterns to match numbers in the specified format\n",
    "    addition_pattern = r'(\\d+)\\s*\\+\\s*(\\d+)'\n",
    "    a_number_pattern = r'a:\\s*(\\d+)'\n",
    "    \n",
    "    # Find and extract numbers for the addition\n",
    "    addition_match = re.search(addition_pattern, content)\n",
    "    if addition_match:\n",
    "        num1, num2 = addition_match.groups()\n",
    "    else:\n",
    "        num1, num2 = None, None\n",
    "\n",
    "    # Find and extract the number following 'a:'\n",
    "    a_number_match = re.search(a_number_pattern, content)\n",
    "    if a_number_match:\n",
    "        a_number = a_number_match.group(1)\n",
    "    else:\n",
    "        a_number = None\n",
    "\n",
    "    return int(num1), int(num2), int(a_number)\n",
    "\n",
    "def run_problem(num1, num2):\n",
    "    eval_prompt = f\" Answer the following:\\n{num1} + {num2}\\na:\"\n",
    "    model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        response = tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def generate_random_n_digit_number(n):\n",
    "    \"\"\"\n",
    "    Generates a random n-digit number.\n",
    "\n",
    "    :param n: Number of digits in the number.\n",
    "    :return: A random n-digit number.\n",
    "    \"\"\"\n",
    "    # Ensure the first digit is not zero\n",
    "    first_digit = random.randint(1, 9)\n",
    "\n",
    "    # Generate the remaining n-1 digits, which can include zero\n",
    "    remaining_digits = [random.randint(0, 9) for _ in range(n - 1)]\n",
    "\n",
    "    # Combine the digits to form the number\n",
    "    digits = [first_digit] + remaining_digits\n",
    "    return int(''.join(map(str, digits)))\n",
    "\n",
    "def get_random_pair(used_pairs: set, i: int, j: int) -> Tuple[int, int]:\n",
    "    nums_used = True\n",
    "    while nums_used:\n",
    "        num1 = generate_random_n_digit_number(i)\n",
    "        num2 = generate_random_n_digit_number(j)\n",
    "        pair = (num1, num2)\n",
    "        nums_used = pair in used_pairs\n",
    "    used_pairs.add(pair)\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c4f655",
   "metadata": {},
   "source": [
    "One example to make sure those functions work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cad5a19b-3f71-4424-8bdf-ee47d5f75bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204332 2819 207151 True\n"
     ]
    }
   ],
   "source": [
    "used_pairs = set()\n",
    "num1, num2 = get_random_pair(used_pairs, 6, 4)\n",
    "response = run_problem(num1, num2)\n",
    "_, _, answer = extract_numbers_from_string(response)\n",
    "print(num1, num2, answer, answer == (num1 + num2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aeb94d3-8418-464c-b68d-65d028a79da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb704b",
   "metadata": {},
   "source": [
    "Now for the batch testing. We'll test for 6 digit LHS, and 1-6 digits RHS. 100 trials for each RHS digit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f795f227-c312-4977-930f-c1e1d61f5421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700133 9 700142 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130564 2 130566 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801543 1 801544 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801065 2 801067 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191699 9 191708 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968011 3 968014 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755698 2 755700 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891769 4 891773 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790967 1 790968 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845299 2 845301 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908313 77 908390 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771902 39 771941 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501399 14 501413 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873782 73 873855 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217240 15 217255 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621456 33 621489 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615850 74 615924 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349985 79 350064 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344550 32 344582 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259696 74 259770 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898079 459 898538 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753794 838 754632 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "898089 424 898513 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384511 644 385155 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423705 995 424690 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376773 692 377465 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755852 498 756340 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833484 444 833928 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843152 842 843994 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807286 890 808176 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699534 3836 703360 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420489 7423 428912 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691803 4974 696777 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761839 9262 771091 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235290 7561 242851 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255792 5991 261783 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333095 9498 342593 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393870 1516 395386 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766188 3414 769592 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906174 5518 911692 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135213 66804 202017 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580396 62239 642625 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367658 71793 439451 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979374 26089 1005463 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123303 17053 140356 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684282 51724 735906 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426033 33098 459131 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266746 86212 352958 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405548 95102 490648 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671983 96384 768367 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143581 228801 372382 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235775 592995 828770 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879082 823184 1702266 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964300 918063 1882363 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142681 918451 1061132 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895527 736556 1632083 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781149 316433 1097582 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173564 857051 1030615 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962095 451863 1413958 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182404 435048 617452 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "used_pairs = set()\n",
    "results = defaultdict(list)\n",
    "for j in range(1, 7):\n",
    "    for k in range(100):\n",
    "        num1, num2 = get_random_pair(used_pairs, 6, j)\n",
    "        response = run_problem(num1, num2)\n",
    "        _, _, answer = extract_numbers_from_string(response)\n",
    "        is_correct = answer == (num1 + num2)\n",
    "        results[j].append((num1, num2, answer, is_correct))\n",
    "        if k % 10 == 0:\n",
    "            print(num1, num2, answer, is_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d5301",
   "metadata": {},
   "source": [
    "Now the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2336c70c-f314-4cb7-a4a6-9c76639f4aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98, 90, 76, 78, 77, 74]\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for j in range(1, 7):\n",
    "    acc = sum([1 for r in results[j] if r[3]])\n",
    "    accuracies.append(acc)\n",
    "\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b8a00",
   "metadata": {},
   "source": [
    "The results from these same trials on the base model were [93, 73, 31, 54, 53, 58].\n",
    "\n",
    "A major improvement from the base model with only a little fine tuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f260b4df-d4e5-4f95-94d4-6001b250611a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE the first time I ran this I got [94, 89, 79, 77, 81, 75] for the accuracies, this is what is reflected in the chart on the poster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a29255-7dbb-42f8-841a-3d20fcd00516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:.conda-jack_conda_env]",
   "language": "python",
   "name": "conda-env-.conda-jack_conda_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
